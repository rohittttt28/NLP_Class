{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPclass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEOxgRS2dSXLRKWPpCntNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohittttt28/NLP_Class/blob/master/NLPclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9taVO9GugVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d583feee-d591-4f46-9f1a-d425fe6a6665"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "plt.style.use(\"ggplot\")\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdnLi443wU4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import random\n",
        "import string\n",
        "import io\n",
        "\n",
        "#Input of csv \n",
        "df=pd.read_csv('https://raw.githubusercontent.com/rohittttt28/News_Scrapy/master/review.csv',usecols=[\"Review Text\"]).rename(columns={\"Review Text\": \"Review\"}).dropna()\n",
        "\n",
        "#df['Review']=df['Review'].map(lambda x :x.lower()).map(lambda x:x.replace(\" \",\"\"))\n",
        "\n",
        "df['id']=[n for n in np.arange(len(df))]\n",
        "df=df.set_index('id')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVxAR2qmj4CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "809b4b42-3de3-4bd1-a317-2f0a0d15974a"
      },
      "source": [
        "data.head()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-304fa4ce4ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA3scvaqkeYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anJ9Z9b_lw8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Review'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnisJ3_RcUsz",
        "colab_type": "text"
      },
      "source": [
        "NLTK \n",
        "step 1: Tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lchr9pBYXx-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "from nltk import word_tokenize,wordpunct_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_iD6lcfY8fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df['Tokens']=df['Review'].map(lambda x:tokenizer.tokenize(x))\n",
        "df['Tokens']=df['Review'].apply(lambda x:tokenizer.tokenize(x))\n",
        "x1=df.drop(['Review'],axis=1).copy(deep=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mBz4_HOZ5YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6PNnW8cjIo",
        "colab_type": "text"
      },
      "source": [
        "STEP 2: Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZvoMuLeaq01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def filter_me_bitch(x):\n",
        "  filtered_sentence = [] \n",
        "  for w in x: \n",
        "      if w not in stop_words: \n",
        "          filtered_sentence.append(w)\n",
        "  return filtered_sentence         \n",
        "x1['Filter_tok']=x1['Tokens'].apply(lambda x:filter_me_bitch(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmwkHaEqeK3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1\n",
        "print(x1['Tokens'][1])\n",
        "print(x1['Filter_tok'][1])\n",
        "print(len(x1['Tokens'][1]))\n",
        "print(len(x1['Filter_tok'][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmsb413bgZit",
        "colab_type": "text"
      },
      "source": [
        "Step 3:Let's stem "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5WC_p2ffhQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer() # is it a ps ,you write after every script ,,,Hell NOOO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P83a1TKPhTcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x1['stem_it']=x1['Filter_tok'].apply(lambda x:ps.stem(x)) #not working because we need a list le me**Happy diwali\n",
        "x1['stem_it']=x1['Filter_tok'].apply(lambda x:stem_lightly(x)) # yah it worked !!!baap baap hota h \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmwG4cMpjW9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1['stem_it']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rvVmY3dj0SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YFJEwBAj-mv",
        "colab_type": "text"
      },
      "source": [
        "Step 4: Lemmatization # spelling sahi samjh lena "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFCqMy0CkRCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet') #download wali cheez pehle kiya kr \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "def lemmatizz_slowly(x):\n",
        "  cd=[]\n",
        "  for i in x:\n",
        "    cd.append(lemmatizer.lemmatize(i))\n",
        "  return cd\n",
        "\n",
        "x1['lemma']=x1['Filter_tok'].apply(lambda x:lemmatizz_slowly(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJviOST8k5Og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1\n",
        "#abhi itna hi ...to be continued!! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYf29sWXlnHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}